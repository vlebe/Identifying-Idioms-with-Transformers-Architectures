{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, CamembertTokenizer, BertTokenizer, DistilBertModel, CamembertModel, BertModel, XLMTokenizer, XLMModel\n",
    "import torch\n",
    "import pandas as pd \n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset2 import MWEDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer import BertMWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "train_dataset = MWEDataset(f\"train_BIGO.csv\", tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "batch = train_loader.__iter__().__next__()\n",
    "\n",
    "model = BertMWE(4, bert_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1601it [20:06,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"test_BIGO.csv\", sep=\"\\t\", \n",
    "                              converters={\"token_list\": ast.literal_eval, \n",
    "                                          \"lemmas\": ast.literal_eval, \n",
    "                                          'labels': ast.literal_eval})\n",
    "\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "count = \n",
    "for idx, row in tqdm(df.iterrows()): \n",
    "    tokens = row.token_list\n",
    "\n",
    "    sentence = \" \".join(tokens)\n",
    "    tokens_bert = tokenizer.tokenize(sentence)\n",
    "\n",
    "    encoding = tokenizer.encode_plus(tokens, add_special_tokens=True, \n",
    "                                max_length=512, padding='max_length', \n",
    "                                return_attention_mask=True, \n",
    "                                return_tensors='pt', truncation=True)\n",
    "    input_ids, attention_mask = encoding['input_ids'], encoding['attention_mask']\n",
    "\n",
    "    embedding = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "    if len(tokens_bert)>512 :\n",
    "        final_emb = embedding\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = torch.load(\"index_list_test.pt\")\n",
    "input1 = torch.load(\"input_id_2000.pt\")\n",
    "input2 = torch.load(\"input_id_4000.pt\")\n",
    "input3 = torch.load(\"input_id_6000.pt\")\n",
    "input4 = torch.load(\"input_id_8000.pt\")\n",
    "input5 = torch.load(\"input_id_10000.pt\")\n",
    "input6 = torch.load(\"input_id_12000.pt\")\n",
    "input7 = torch.load(\"input_id_14000.pt\")\n",
    "input8 = torch.load(\"input_id_fin.pt\")\n",
    "\n",
    "input = torch.cat((input1, input2, input3, input4, input5, input6, input7, input8), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1846"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.load(\"embeddings/test/embeddings_tensor_test_IGO_2000.pt\")\n",
    "len(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13398, 13398)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id), len(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.load('embeddings/test/embeddings_tensor_test_BIGO_2000.pt')\n",
    "\n",
    "for i in range(len(test)) :\n",
    "    if torch.equal(test[i], final_emb[0]) :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6]) torch.Size([4]) torch.Size([4, 4])\n",
      "tensor([[0, 1, 3, 2, 3, 0],\n",
      "        [0, 1, 3, 2, 3, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def viterbi(emission_matrices, initial_proba, transition_proba):\n",
    "    B, N, T = emission_matrices.shape\n",
    "\n",
    "    delta = torch.zeros(B, N, T)\n",
    "    psi = torch.zeros(B, N, T)\n",
    "\n",
    "    # Initialisation\n",
    "    delta[:, :, 0] = emission_matrices[:, :, 0] * initial_proba.view(1, -1)\n",
    "\n",
    "    # Calcul des différentes valeurs de delta\n",
    "    for t in range(1, T):\n",
    "        for j in range(N):\n",
    "            liste = [\n",
    "                delta[b, i, t-1] * transition_proba[i, j] * emission_matrices[b, j, t]\n",
    "                for b in range(B)\n",
    "                for i in range(N)\n",
    "            ]\n",
    "            liste = torch.tensor(liste).view(B, N)\n",
    "            delta[:, j, t], psi[:, j, t] = liste.max(dim=1)\n",
    "\n",
    "    # Détermination du meilleur chemin\n",
    "    z_T = delta[:, :, T - 1].argmax(dim=1)\n",
    "    z = torch.zeros(B, T, dtype=torch.long)\n",
    "    z[:, T - 1] = z_T\n",
    "\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        z[:, t] = psi[torch.arange(B), z[:, t + 1], t + 1]\n",
    "\n",
    "    return z\n",
    "\n",
    "# Example usage\n",
    "initial_proba = torch.tensor([0.25, 0.25, 0.25, 0.25])\n",
    "transition_proba = torch.rand(4, 4)\n",
    "transition_proba = F.softmax(transition_proba, dim=1)\n",
    "\n",
    "input_seq = ['le', 'chat', 'adore', 'le', 'lait', '.']\n",
    "emission_proba = torch.rand(len(input_seq), 4).T\n",
    "emission_proba[:, 0] = torch.tensor([1, 0, 0, 0])\n",
    "emission_proba[:, 1] = torch.tensor([0, 1, 0, 0])\n",
    "emission_proba[:, 2] = torch.tensor([0, 0, 0, 1])\n",
    "emission_proba[:, 3] = torch.tensor([-1, 0, 1, 0])\n",
    "emission_proba[:, 4] = torch.tensor([0, 0, 0, 1])\n",
    "emission_proba[:, 5] = torch.tensor([1, 0, 0, 0])\n",
    "\n",
    "emission_proba = emission_proba.unsqueeze(0)\n",
    "emission_proba = torch.cat((emission_proba, emission_proba), dim=0)\n",
    "emission_proba = F.softmax(emission_proba, dim=1)\n",
    "\n",
    "print(emission_proba.shape, initial_proba.shape, transition_proba.shape)\n",
    "print(viterbi(emission_proba, initial_proba, transition_proba))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(emission_matrix, initial_proba, transition_proba) :\n",
    "    shape = emission_matrix.shape\n",
    "\n",
    "    delta = torch.zeros(shape)\n",
    "    psi = torch.zeros(shape)\n",
    "\n",
    "    # Initialisation\n",
    "    delta[:, 0] = emission_matrix[:, 0] * initial_proba\n",
    "\n",
    "    # Calcul des différentes valeurs de delta\n",
    "    for t in range(0, shape[1] - 1) :\n",
    "        for j in range(shape[0]) :\n",
    "            liste = [ delta[i, t] * transition_proba[i, j] * emission_matrix[j, t + 1] for i in range(shape[0])]\n",
    "            delta[j, t + 1], psi[j, t + 1] = max(liste), torch.argmax(torch.tensor(liste))\n",
    "\n",
    "    # Détermination du meilleur chemin \n",
    "    z_T = torch.argmax(torch.tensor([delta[i, shape[1]-1] for i in range(shape[0])])).long()\n",
    "    z = torch.zeros(shape[1]).long()\n",
    "    z[shape[1]-1] = z_T\n",
    "    for t in range(shape[1]-2, 0, -1) :\n",
    "        z[t] = psi[z[t+1], t+1]\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6]) torch.Size([4]) torch.Size([4, 4])\n",
      "tensor([[0, 1, 3, 2, 0, 0],\n",
      "        [0, 1, 3, 2, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from viterbi import get_initial_proba, get_transition_proba\n",
    "import torch.nn.functional as F\n",
    "\n",
    "initial_proba = torch.tensor(get_initial_proba(\"train_BIGO.csv\"))\n",
    "transition_proba = torch.tensor(get_transition_proba(\"train_BIGO.csv\"))\n",
    "\n",
    "\n",
    "input = ['le', 'chat', 'adore', 'le', \"lait\", '.']\n",
    "emission_proba = torch.rand(len(input), 4).T\n",
    "emission_proba[:, 0] = torch.tensor([1, -1, -1, 2])\n",
    "emission_proba[:, 1] = torch.tensor([-1, 1, -1, -1])\n",
    "emission_proba[:, 2] = torch.tensor([-1, -1, -1, 1])\n",
    "emission_proba[:, 3] = torch.tensor([-1, -1, 1, -1])\n",
    "emission_proba[:, 4] = torch.tensor([1, -1, -1, -1])\n",
    "emission_proba[:, 5] = torch.tensor([1, -1, -1, 2])\n",
    "\n",
    "emission_proba = emission_proba.unsqueeze(0)\n",
    "emission_proba = torch.cat((emission_proba, emission_proba), dim=0)\n",
    "emission_proba = F.softmax(emission_proba, dim=2)\n",
    "# emission_proba = F.softmax(emission_proba, dim=1)\n",
    "print(emission_proba.shape, initial_proba.shape, transition_proba.shape)\n",
    "\n",
    "print(viterbi(emission_proba, initial_proba, transition_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "cam = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "bert = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "xlm = XLMTokenizer.from_pretrained('xlm-mlm-enfr-1024')\n",
    "\n",
    "# dataset = MWEDataset('test_BIGO.csv', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tok = 0\n",
    "max_ind = 0\n",
    "max_lab = 0\n",
    "for i in range(len(dataset)) :\n",
    "    _, _, tokens, indices, labels = dataset[i]\n",
    "    if len(tokens) > max_tok :\n",
    "        max_tok = len(tokens)\n",
    "    if len(indices) > max_ind :\n",
    "        max_ind = len(indices)\n",
    "    if len(labels) > max_lab :\n",
    "        max_lab = len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 580, 400)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tok, max_ind, max_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"l'\", 'ananas', 'flambé', 'au', 'à', 'le', 'rhum', 'ne', 'venait', 'pas', \"d'\", 'une', 'boite', 'de', 'conserve', '.']\n",
      "16\n",
      "[5, 185, 24852, 3, 862, 169, 185, 3, 324, 13319, 2475, 234, 59, 3, 234, 3, 9, 6]\n",
      "18\n",
      "['▁le', '▁', 'ananas', '▁fla', 'mber', '▁_', '▁à', '▁le', '▁rhum', '▁ne', '▁venir', '▁pas', '▁de', '▁un', '▁boîte', '▁de', '▁conserve', '▁', '.']\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train_BIGO.csv', sep=\"\\t\", \n",
    "                              converters={\"token_list\": ast.literal_eval, \n",
    "                                          \"lemmas\": ast.literal_eval, \n",
    "                                          'labels': ast.literal_eval})\n",
    "\n",
    "tokens = df.token_list.iloc[0]\n",
    "print(tokens)\n",
    "lemmas = df.lemmas.iloc[0]\n",
    "print(len(lemmas))\n",
    "\n",
    "enc = cam.encode_plus(lemmas)\n",
    "print(enc[\"input_ids\"])\n",
    "print(len(enc[\"input_ids\"]))\n",
    "\n",
    "sentence = \" \".join(lemmas)\n",
    "encoding = cam.tokenize(sentence)\n",
    "print(encoding)\n",
    "\n",
    "labels = df.labels.iloc[0]\n",
    "print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 'on'\n",
    "token in ('ce', 'on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_tokens(tokens : list[str]): \n",
    "            list_mapping, counter, is_dash, is_parenthesis, is_digits, count_dash = [0], 0, False, False, False, 0\n",
    "            for idx, token in enumerate(tokens):\n",
    "                if not(token.startswith(\"##\")):\n",
    "                    if token == '[UNK]' :\n",
    "                        token = \"'\"\n",
    "                    elif token == \"hui\" and tokens[idx-1] == \"'\":\n",
    "                        continue\n",
    "                    \n",
    "                    elif \"'\" in token: \n",
    "                        is_dash = False\n",
    "                    elif is_dash:\n",
    "                        if token in (\"ce\", \"on\"): \n",
    "                            counter += 1\n",
    "                            is_dash = False\n",
    "                        elif token == \"-\":\n",
    "                            count_dash += 1\n",
    "                            is_dash=True\n",
    "                        else:\n",
    "                            is_dash=False\n",
    "                            count_dash = 0\n",
    "\n",
    "                        if count_dash == 2 :\n",
    "                            counter +=1 \n",
    "                            is_dash = False\n",
    "                            count_dash = 0\n",
    "                            list_mapping[-1] += 1\n",
    "\n",
    "                    elif \"-\" in token:\n",
    "                        if idx == 0 :\n",
    "                            counter += 1\n",
    "                        elif tokens[idx-1] == \"a\":\n",
    "                            counter += 1\n",
    "                            is_dash = True\n",
    "                            count_dash += 1\n",
    "                        else : \n",
    "                            is_dash = True\n",
    "                            count_dash += 1\n",
    "                            \n",
    "                    elif token in (\"(\", \")\"): \n",
    "                        if token == \"(\": \n",
    "                            token_after = tokens[idx+1]\n",
    "                            if len(token_after) >= 2: \n",
    "                                counter += 1\n",
    "                            elif token_after.isdigit():\n",
    "                                counter += 1\n",
    "                            else : \n",
    "                                is_parenthesis = True\n",
    "                        else: \n",
    "                            token_before = tokens[idx-1]\n",
    "                            if len(token_before) >= 2: \n",
    "                                counter += 1\n",
    "                            is_parenthesis = False\n",
    "                    elif is_parenthesis: \n",
    "                        is_parenthesis=False\n",
    "\n",
    "                    elif token.isdigit():\n",
    "                        if not is_digits:\n",
    "                            counter += 1\n",
    "                            is_digits = True\n",
    "                    elif is_digits:\n",
    "                        if not (token == \",\" and tokens[idx+1].isdigit()) :\n",
    "                            is_digits = False\n",
    "                            counter += 1\n",
    "\n",
    "                    elif token == '.' :\n",
    "                        if tokens[idx-1] in (\"etc\", \"cf\", '.'):\n",
    "                            continue\n",
    "\n",
    "                        else :\n",
    "                            counter += 1\n",
    "\n",
    "                    else :\n",
    "                        counter += 1\n",
    "                list_mapping.append(counter)\n",
    "            return list_mapping\n",
    "\n",
    "tokens_test = ['Le', 'cha', \"##t\", \"d\", \"''\", \"élevage\", \"traité\", \"(\", \"e\", \")\", \"test\", \"(\", \"vieux\", \"monsieur\", \")\", 'su', '##ffi', '##xe', '-', '-', 'villa']\n",
    "tokens_test = [\"Arb\", '##re', '7', '736', \"890\", \"6\", 'su', 'test', '7', '786', \"fin\"]\n",
    "tokens_test = [\"Saint\", \"-\", \"germain\", 'su', '##ffi', '##xe', '-', '-', 'villa']\n",
    "list_mapping = mapping_tokens(tokens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 2, 2, 2, 3, 3, 4]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset2 import MWEDataset\n",
    "\n",
    "dataset = MWEDataset('train_BIGO.csv', tokenizer=tokenizer)\n",
    "\n",
    "def create_mapped_lists(second_tokens, mapping):\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(mapping)) :\n",
    "        if i > 0 :\n",
    "            if len(result)== 0 :\n",
    "                result.append([second_tokens[i-1]])\n",
    "            elif mapping[i] == mapping[i-1] :\n",
    "                result[-1].append(second_tokens[i-1])\n",
    "            else :\n",
    "                result.append([second_tokens[i-1]])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Il'], ['était'], ['construit'], ['par'], ['les'], ['ateliers'], ['La'], ['Prairie'], ['à'], ['L', \"'\"], ['Isle', '-', 'd', \"'\"], ['Es', '##pagna', '##c'], ['('], ['Charente'], [')'], [','], ['spécial', '##isés'], ['dans'], ['la'], ['fabrication'], ['de'], ['matériel'], ['de'], ['camping'], [';']]\n",
      "['Il', 'était', 'construit', 'par', 'les', 'ateliers', 'La', 'Prairie', 'à', \"L'\", \"Isle-d'Espagnac\", '(', 'Charente', ')', ',', 'spécialisés', 'dans', 'la', 'fabrication', 'de', 'matériel', 'de', 'camping', ';']\n",
      "[['Les'], ['en', '##co', '##ches'], ['ou'], ['c', '##rans'], ['sur'], ['les'], ['plan', '-', 'films'], ['permettent'], ['d', \"'\"], ['ide', '##nti', '##fier'], ['les'], ['marques'], ['et'], ['types'], ['d', \"'\"], ['é', '##mul', '##sion'], ['('], ['sens', '##ibil', '##ité'], [','], ['co', '##uc', '##leur'], [','], ['noir'], ['&'], ['blanc'], [','], ['in', '##versi', '##ble'], [')']]\n",
      "['Les', 'encoches', 'ou', 'crans', 'sur', 'les', 'plan-films', 'permettent', \"d'\", 'identifier', 'les', 'marques', 'et', 'types', \"d'\", 'émulsion', '(', 'sensibilité', ',', 'coucleur', ',', 'noir', '&', 'blanc', ',', 'inversible', ')', '.']\n",
      "[['Ka', '##st', '##ner'], ['débute'], ['sa'], ['carrière'], ['d', \"'\"], ['agent'], ['artistique'], ['chez'], ['Music'], ['Corporation'], ['of'], ['America'], ['('], ['MCA'], [')']]\n",
      "['Kastner', 'débute', 'sa', 'carrière', \"d'\", 'agent', 'artistique', 'chez', 'Music', 'Corporation', 'of', 'America', '(', 'MCA', ')', '.']\n",
      "[['Il'], ['fut'], ['aussi'], ['«'], ['un'], ['excellent'], ['observa', '##teur'], ['des'], ['de'], ['les'], ['mouvements'], ['musica', '##ux'], ['qui'], ['travers', '##èrent'], ['son'], ['temps'], ['»'], ['('], ['Guy'], ['Er', '##isma', '##nn'], [')']]\n",
      "['Il', 'fut', 'aussi', '«', 'un', 'excellent', 'observateur', 'des', 'de', 'les', 'mouvements', 'musicaux', 'qui', 'traversèrent', 'son', 'temps', '»', '(', 'Guy', 'Erismann', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "lab_count = 0\n",
    "nb_sentences = 0\n",
    "for i in range(len(dataset)) :\n",
    "    nb_sentences += 1\n",
    "    tokens, bert_tokens, labels = dataset[i]\n",
    "    list_mapping = mapping_tokens(bert_tokens)\n",
    "    if max(list_mapping) != len(labels) - 2 :\n",
    "        results = create_mapped_lists(bert_tokens, list_mapping)\n",
    "        print(results)\n",
    "        print(tokens)\n",
    "        count += 1\n",
    "        if count == 4 :\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "lab_count = 0\n",
    "nb_sentences = 0\n",
    "for i in range(len(dataset)) :\n",
    "    nb_sentences += 1\n",
    "    tokens, bert_tokens, labels = dataset[i]\n",
    "    list_mapping = mapping_tokens(bert_tokens)\n",
    "    if max(list_mapping) != len(labels) -2 :\n",
    "        count += 1\n",
    "        for elt in labels :\n",
    "            if elt != 0 and elt != -100 :\n",
    "                lab_count += 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1142, 14540, 304)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, nb_sentences, lab_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 2, 3, 3, 4, 5, 5, 5, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -1,  -1,  -1],\n",
      "        [  1,   2,   3],\n",
      "        [  1,   2,   3],\n",
      "        [ -1,  -2,  -3],\n",
      "        [  4,   5,   6],\n",
      "        [  7,   8,   9],\n",
      "        [ 10,  11,  12],\n",
      "        [ 13,  14,  15],\n",
      "        [-13, -14, -15],\n",
      "        [ 16,  17,  18],\n",
      "        [-10, -10, -10],\n",
      "        [-10, -10, -10],\n",
      "        [-10, -10, -10]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_6540\\4261732794.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings_final = torch.tensor(new_line.detach().to(device)).unsqueeze(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.,  -1.,  -1.],\n",
       "        [  1.,   2.,   3.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  4.,   5.,   6.],\n",
       "        [  7.,   8.,   9.],\n",
       "        [ 10.,  11.,  12.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [ 16.,  17.,  18.],\n",
       "        [-10., -10., -10.],\n",
       "        [-10., -10., -10.],\n",
       "        [-10., -10., -10.]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def mean_embed(list_mapping : list[int], embeddings : torch.Tensor):\n",
    "            current_map, list_current_embed, embeddings_final = None, None, None\n",
    "            for idx, mapping in enumerate(list_mapping): \n",
    "\n",
    "                if idx==0: \n",
    "                    current_map = mapping\n",
    "                    list_current_embed = [embeddings[idx, :]]\n",
    "\n",
    "                elif idx==len(list_mapping)-1:\n",
    "                    if current_map == mapping:\n",
    "                        list_current_embed.append(embeddings[idx, :])\n",
    "                    else: \n",
    "                        mean_embeddings = None\n",
    "                        for idx_embed, embed in enumerate(list_current_embed): \n",
    "                                if idx_embed==0: \n",
    "                                    mean_embeddings = embed\n",
    "                                else : \n",
    "                                    mean_embeddings += embed\n",
    "                        new_line = mean_embeddings/len(list_current_embed)\n",
    "                        embeddings_final = torch.cat((embeddings_final, new_line.detach().unsqueeze(0).to(device)), dim=0)\n",
    "                        list_current_embed = [embeddings[idx, :]]\n",
    "                    mean_embeddings = None\n",
    "                    for idx_embed, embed in enumerate(list_current_embed): \n",
    "                            if idx_embed==0: \n",
    "                                mean_embeddings = embed\n",
    "                            else : \n",
    "                                mean_embeddings += embed \n",
    "                    new_line = mean_embeddings/len(list_current_embed)\n",
    "                    embeddings_final = torch.cat((embeddings_final, new_line.detach().unsqueeze(0).to(device), embeddings[idx+1:,:].to(device)), dim=0)\n",
    "\n",
    "                elif current_map != mapping:\n",
    "                    mean_embeddings = None\n",
    "                    for idx_embed, embed in enumerate(list_current_embed): \n",
    "                            if idx_embed==0: \n",
    "                                mean_embeddings = embed\n",
    "                            else : \n",
    "                                mean_embeddings += embed\n",
    "                    if embeddings_final is not None:\n",
    "                        new_line = mean_embeddings/len(list_current_embed)\n",
    "                        embeddings_final = torch.cat((embeddings_final, new_line.detach().unsqueeze(0).to(device)), dim=0)\n",
    "                    else: \n",
    "                        new_line = mean_embeddings/len(list_current_embed)\n",
    "                        embeddings_final = torch.tensor(new_line.detach().to(device)).unsqueeze(0)\n",
    "\n",
    "                    list_current_embed = [embeddings[idx, :]]\n",
    "                    current_map = mapping\n",
    "\n",
    "                else : \n",
    "                    list_current_embed.append(embeddings[idx, :])\n",
    "\n",
    "            return embeddings_final\n",
    "\n",
    "embeddings_tensor = torch.tensor([[-1, -1, -1], [1, 2, 3], [1, 2, 3], [-1, -2, -3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [-13, -14, -15], [16, 17, 18], [-10, -10, -10],  [-10, -10, -10],  [-10, -10, -10]])\n",
    "mean_embed(list_mapping, embeddings_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\Desktop\\PSTALN\\test.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexa/Desktop/PSTALN/test.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m [[\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m]]\u001b[39m.\u001b[39;49msqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "[[1,2,3]].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"l'\", 'ananas', 'flambé', 'au', 'à', 'le', 'rhum', 'ne', 'venait', 'pas', \"d'\", 'une', 'boite', 'de', 'conserve', '.']\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(row[\"token_list\"])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m CrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;241m0\u001b[39m], batch[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\lebev\\Desktop\\3A\\PSTALN\\PSTALN\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\lebev\\Desktop\\3A\\PSTALN\\PSTALN\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\lebev\\Desktop\\3A\\PSTALN\\PSTALN\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lebev\\Desktop\\3A\\PSTALN\\PSTALN\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lebev\\Desktop\\3A\\PSTALN\\PSTALN\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lebev\\Desktop\\3A\\PSTALN\\PSTALN\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lebev\\Desktop\\3A\\PSTALN\\PSTALN\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:138\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from create_embeddings import MWEDataset\n",
    "from transformer import CamembertMWE\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "camembert_model = CamembertModel.from_pretrained('camembert-base')\n",
    "\n",
    "learning_rate = 5e-5\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "test_dataset = MWEDataset(\"test_BIGO.csv\", tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = CamembertMWE(4, camembert_model, \"cpu\")\n",
    "\n",
    "for param in model.bert.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "loss = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "logits = model(batch[0], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\u001b[43mlogits\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(logits, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpreds\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_by_class(y_true, y_pred):\n",
    "    # Filter out cases where y_true is -100\n",
    "    valid_indices = [i for i, label in enumerate(y_true) if label != -100]\n",
    "    y_true_valid = np.array([y_true[i] for i in valid_indices])\n",
    "    y_pred_valid = np.array([y_pred[i] for i in valid_indices])\n",
    "\n",
    "    # Calculate F1 score by class\n",
    "    f1_scores = f1_score(y_true_valid, y_pred_valid, average=None, labels=np.unique(y_true_valid))\n",
    "\n",
    "    # Calculate accuracy score by class\n",
    "    accuracy_scores = accuracy_score(y_true_valid, y_pred_valid, normalize=False)\n",
    "    total_valid_samples = len(y_true_valid)\n",
    "    accuracy_scores /= total_valid_samples\n",
    "\n",
    "    return f1_scores, accuracy_scores\n",
    "\n",
    "# Example usage:\n",
    "y_true = [-100, 0, 0, 0, 1, 2, 3, 0, 0, 0, -100]\n",
    "y_pred = [1, 3, 2, 0, 0, 0, 1, 3, 0, 0, 0]\n",
    "\n",
    "f1_scores, accuracy_scores = calculate_scores_by_class(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_scores_by_class(y_true, y_pred):\n",
    "    # Filter out cases where y_true is -100\n",
    "    valid_indices = [i for i, label in enumerate(y_true) if label != -100]\n",
    "    y_true_valid = np.array([y_true[i] for i in valid_indices])\n",
    "    y_pred_valid = np.array([y_pred[i] for i in valid_indices])\n",
    "\n",
    "    # Calculate F1 score by class\n",
    "    f1_scores = f1_score(y_true_valid, y_pred_valid, average=None, labels=np.unique(y_true_valid))\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "# Example usage:\n",
    "y_true = [-100, 0, 0, 0, 1, 2, 3, 0, 0, 0, -100]\n",
    "y_pred = [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "f1_scores = calculate_scores_by_class(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92307692, 0.66666667, 0.        , 0.        ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_BIGO.csv', sep=\"\\t\", \n",
    "                              converters={\"token_list\": ast.literal_eval, \n",
    "                                          \"lemmas\": ast.literal_eval, \n",
    "                                          'labels': ast.literal_eval})\n",
    "\n",
    "labels = df.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = 0\n",
    "B = 0\n",
    "I = 0\n",
    "G = 0\n",
    "for label in labels :\n",
    "    for l in label :\n",
    "        if l == 0 :\n",
    "            O += 1\n",
    "        elif l == 1 :\n",
    "            B += 1\n",
    "        elif l == 2 :\n",
    "            I += 1\n",
    "        elif l == 3 :\n",
    "            G += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352744 3865 5003 2802\n"
     ]
    }
   ],
   "source": [
    "print(O, B, I, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.1647543052400664, 98.92802365281834, 98.61239387711518, 99.22285181764475)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "somme = O + I + G\n",
    "100-(O*100/somme), 100-(B*100/somme), 100-(I*100/somme), 100-(G*100/somme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: tensor([[1, 2],\n",
      "        [1, 2]])\n",
      "Inputs: tensor([[0.1000, 0.2000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.5000]])\n",
      "Tests: tensor([[0.0100, 0.0200, 0.0300],\n",
      "        [0.0400, 0.0500, 0.3000]])\n",
      "Shapes - Labels: torch.Size([2, 2]) Inputs: torch.Size([2, 3]) Tests: torch.Size([2, 3])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.labels = data['labels']\n",
    "        self.inputs = data['inputs']\n",
    "        self.tests = data['tests']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # Assuming all lists have the same length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        input_data = torch.tensor(self.inputs[idx])\n",
    "        test_data = torch.tensor(self.tests[idx])\n",
    "        return label, input_data, test_data\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'labels': [[1, 2], [1, 2]],\n",
    "    'inputs': [[0.1, 0.2, .3], [0.3, 0.4, 0.5], [0.6, 0.7, 0.8]],\n",
    "    'tests': [[0.01, 0.02, 0.03], [0.04, 0.05, .3], [0.06, 0.07, 0.08]]\n",
    "}\n",
    "\n",
    "# Create dataset and dataloader\n",
    "my_dataset = MyDataset(data)\n",
    "batch_size = 2\n",
    "my_dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through batches\n",
    "for batch in my_dataloader:\n",
    "    labels, inputs, tests = batch\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Inputs:\", inputs)\n",
    "    print(\"Tests:\", tests)\n",
    "    print(\"Shapes - Labels:\", labels.shape, \"Inputs:\", inputs.shape, \"Tests:\", tests.shape)\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
